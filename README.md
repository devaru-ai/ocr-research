# BARTX: Fast, Data-Efficient Text Line Recognition with Transformer LM

## ğŸ§  Overview

**BARTX** is a lightweight, modular system for post-OCR correction in noisy or low-resource document scenarios. Instead of relying on massive autoregressive OCR models like TrOCR or DTrOCR, BARTX combines a parallel, detection-based pipeline with a fine-tuned **BART** language model to enhance raw OCR outputsâ€”dramatically improving both accuracy and speed.

This research targets a common bottleneck in document intelligence: the **fragility of OCR outputs in handwritten, historical, or degraded texts**. With just ~16K synthetic training lines, BARTX demonstrates that smart architectureâ€”not scaleâ€”can drive performance.

---

## ğŸ” Why It Matters

OCR failures are often the weak link in downstream NLP tasks like entity extraction, summarization, or classification. Rather than build an end-to-end model, BARTX **surgically improves OCR text post hoc** using a Transformer LMâ€”yielding higher text fidelity, faster inference, and edge-readiness without retraining detection models.

---

## ğŸš€ Key Highlights

- âš¡ **76% Faster than TrOCR**  
  â†’ *67 ms per line* (A6000, batch=1) vs *284 ms for TrOCR*

- ğŸ“‰ **Competitive CER/WER** on noisy real-world handwriting

- ğŸ§  **Compact Architecture**  
  â†’ ~139M parameters (Detector + BART-base)

- ğŸ“¦ **Edge-Deployable**  
  â†’ Parallel, modular, low memory footprint

---

## ğŸ“Š Comparison with SOTA

| Metric                | TrOCR             | DTrOCR         | **BARTX (Ours)**              |
|-----------------------|-------------------|----------------|-------------------------------|
| **Training Data**     | 684Mâ€“1B+ lines     | Billions       | ~16K lines (IAM synthetic)    |
| **Inference Speed**   | 284 ms/line       | 70â€“150 ms (est)| 69 ms/line (batch=1)          |
| **Model Size**        | 558M (TrOCR-Large)| 105M           | 139M (BART-base + lightweight detector) |
| **Deployment**        | Limited           | Moderate       | âœ” High (parallel + modular)   |

---

## ğŸ“Œ Status

**Ongoing thesis-linked research** â€” architectural details and full results will be released post-publication.

---

## ğŸ· Keywords

`OCR correction`, `sequence-to-sequence`, `BART`, `document intelligence`, `low-resource NLP`, `inference optimization`, `edge deployment`


